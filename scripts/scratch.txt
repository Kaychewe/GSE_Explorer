#!/bin/bash

# TODO: track memory usage into csv 
# Initial filesize
# Intermidiate 
# Final size 
# Clear memory 

eval "$(conda shell.bash hook)"
conda activate bsseq_env

# UTILITY FUNCTIONS 
# Mount F: if needed
# TODO turn into a function
if ! mount | grep -q "/mnt/f"; then
    echo "ðŸ“¦ Mounting F: drive..."
    sudo mount -t drvfs F: /mnt/f
fi

timestamp=$(date +"%m-%d-%Y %H:%M:%S")
mkdir -p logs
LOG_FILE="logs/log_alignment_${timestamp}.txt"

message() {
    # Function for timestamp message
    echo "[$(date +"%m-%d-%Y %H:%M:%S")]: $1" | tee -a "$LOG_FILE"
}

# Create reference genome directory 
mkdir -p /mnt/f/Research_Drive/01.raw/01.methylation/fastq/ref
mkdir -p /mnt/f/Research_Drive/01.raw/01.methylation/fastq/ref/hg38
cd /mnt/f/Research_Drive/01.raw/01.methylation/fastq/ref/hg38


GENOMEz=/mnt/f/Research_Drive/01.raw/01.methylation/fastq/ref/hg38/hg38.fa.gz

wget -O $GENOMEz http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz
gunzip $GENOMEz

GENOME_DIR=/mnt/f/Research_Drive/01.raw/01.methylation/fastq/ref/hg38


# TODO CHECK IF ALREADY INDEXED
bismark_genome_preparation --verbose $GENOME_DIR


# TODO (IDEA)
# THE BEST WAY TO READ IN FILES IS BY TAKING IN A .TXT FILE IF MULIPLE FILES 
# UPDATE FOR LINE IN TXT MATCH FIND PAIRS, PASS PARAMS TO BISMARK 
# SAVE MATCHING PATTERN FUNCTION TO HELPERS SCRIPT LATER 
REF_DIR=/mnt/f/Research_Drive/01.raw/01.methylation/fastq/ref/hg38

# SAMPLE-1 Cell-line TE5
INPUT_DIR=/mnt/f/Research_Drive/01.raw/01.methylation/fastq
OUTPUT_DIR=/mnt/f/Research_Drive/01.raw/01.methylation/aligned

bismark \
  --genome "$REF_DIR" \
  -1 "$INPUT_DIR/SRR20731213_1.fastq" \
  -2 "$INPUT_DIR/SRR20731213_2.fastq" \
  -o "$OUTPUT_DIR" \
  --bowtie2 \
  --multicore 8


## Planned bs-SNP
# 1. install samtoool (sort BAM files, mark duplicates etc)
# 2. picard add read groups Add read groups (required for Bis-SNP):
# 3. index the BAM 
# 4. run Bis-SNP Now it's ready for Bis-SNP, which uses
# 5, create a seperate bisulfite-SNP detection git repo (setup pipeline, autodownload data + genome, run pipeline, generate reports, delete data)

